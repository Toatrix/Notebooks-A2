




















from selenium import webdriver #importar el módulo webdriver de la biblioteca selenium.





from bs4 import BeautifulSoup as bs # del modulo bs4  importando la clase BeautifulSoup





import pandas as pd





from selenium.webdriver.chrome.service import Service





from webdriver_manager.chrome import ChromeDriverManager





from urllib.parse import urljoin #util para juntar 2 url´s, util cuando un catalogo no cabe en una pagina y hay poner pag siguiente








from selenium.webdriver.chrome.options import Options
# Especificar la ruta del ejecutable de Brave
brave_path = "C:/Users/usuario/AppData/Local/BraveSoftware/Brave-Browser/Application/brave.exe"
driver_path = "C:/Program Files/chromedriver-win64/chromedriver.exe"  # Asegúrate de que esta es la ruta donde extrajiste el chromedriver.exe


# Configurar opciones de Chrome para usar Brave
chrome_options = Options()
chrome_options.binary_location = brave_path





driver = webdriver.Chrome(service=Service(driver_path), options=chrome_options)#este codigo cambia ya que uso brave en chrome seria diferente,abre una ventena, controlado por selenium





titulosucio=[]
preciosucio=[]
stocksucio=[]


titulos=[]
precios=[]
stock=[]





driver.get("https://www.inmuebles24.com/terrenos-en-venta-en-fraccionamiento-bugambilias.html")





driver.get("https://books.toscrape.com/")








contenido=driver.page_source





soup=bs(contenido)






soup.find_all("article",attrs={"class":"product_pod"})#sacar todos los articulos
        


for i in soup.find_all("article",attrs={"class":"product_pod"}):#itera por los articles y los extrae los datos
    
    titulo=i.find("h3")
    titulosucio.append(titulo.text)
    
    precio= i.find("p",attrs={"class":"price_color"})
    preciosucio.append(precio.text)
    
    stock=i.find("p",attrs={"class":"instock availability"})
    stocksucio.append(stock.text)
    


stocksucio


for i in titulosucio:
    titulos.append(i.replace("\n","").strip())


titulos





for i in preciosucio:
    precios.append(i.replace("\n","").strip())


precios





for i in stocksucio:
    stock.append(i.replace("\n" ,"").strip())


stock











df=pd.DataFrame({"Titulo":titulos,"Precio":precios,"Stock":stock})
df.head(20)





df.to_csv("libros.csv",index=False,encoding="utf-8")





# Importamos las librerías, CODIGO PARA CHROME NO ME SIRVE YA QUE USO BRAVE

import pandas as pd
from selenium import webdriver
from bs4 import BeautifulSoup as bs
from selenium.webdriver.chrome.service import Service
from webdriver_manager.chrome import ChromeDriverManager
from urllib.parse import urljoin

# Configuración

# Configuramos el driver de chrome
driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()))

titulos = []
precios = []
stocks = []

url = "http://books.toscrape.com"

while True:
    titulos_sucios = []
    precios_sucios = []
    stocks_sucio = []

    # urlaux = urljoin(url,f"/catalogue/page-{page}.html")

    driver.get(url)

    # Extracción de datos
    contenido = driver.page_source

    soup = bs(contenido)

    articles = soup.find_all("article",attrs={"class":"product_pod"})

    for libro in articles:
        titulo = libro.find("h3")
        titulos_sucios.append(titulo.text)

        precio = libro.find("p",attrs={"class":"price_color"})
        precios_sucios.append(precio.text)

        stock = libro.find("p",attrs={"class":"instock availability"})
        stocks_sucio.append(stock.text)

    # Limpiando los datos
    for libro in titulos_sucios:
        titulos.append(libro.replace("\n","").strip())

    for libro in precios_sucios:
        precios.append(libro.replace("\n","").strip())

    for libro in stocks_sucio:
        stocks.append(libro.replace("\n","").strip())
    
    siguientepag = soup.select_one("li.next>a")
    if siguientepag:
        siguienteurl = siguientepag.get("href")
        url = urljoin(url, siguienteurl)
    else:
        break


# Almacenamiento
df = pd.DataFrame({"titulo":titulos,"precio":precios,"stock":stocks})
df.head()

# Exportando los datos a un .csv
df.to_csv("libros.csv", index=False, encoding="utf-8")


# Importamos las librerías

import pandas as pd
from selenium import webdriver
from bs4 import BeautifulSoup as bs
from selenium.webdriver.chrome.service import Service
from webdriver_manager.chrome import ChromeDriverManager
from urllib.parse import urljoin

# Configuración

from selenium.webdriver.chrome.options import Options
# Especificar la ruta del ejecutable de Brave
brave_path = "C:/Users/usuario/AppData/Local/BraveSoftware/Brave-Browser/Application/brave.exe"
driver_path = "C:/Program Files/chromedriver-win64/chromedriver.exe"  # Asegúrate de que esta es la ruta donde extrajiste el chromedriver.exe


# Configurar opciones de Chrome para usar Brave
chrome_options = Options()
chrome_options.binary_location = brave_path

driver = webdriver.Chrome(service=Service(driver_path), options=chrome_options)#este codigo cambia ya que uso brave en chrome seria diferente,abre una ventena, controlado por selenium
# Configuramos el driver de chrome
#driver = webdriver.Chrome(service=Service(ChromeDriverManager().install())), INBILITA ES PARA CHROME

titulos = []
precios = []
stocks = []

url = "http://books.toscrape.com"

while True:
    titulos_sucios = []
    precios_sucios = []
    stocks_sucio = []

    # urlaux = urljoin(url,f"/catalogue/page-{page}.html")

    driver.get(url)

    # Extracción de datos
    contenido = driver.page_source

    soup = bs(contenido)

    articles = soup.find_all("article",attrs={"class":"product_pod"})

    for libro in articles:
        titulo = libro.find("h3")
        titulos_sucios.append(titulo.text)

        precio = libro.find("p",attrs={"class":"price_color"})
        precios_sucios.append(precio.text)

        stock = libro.find("p",attrs={"class":"instock availability"})
        stocks_sucio.append(stock.text)

    # Limpiando los datos
    for libro in titulos_sucios:
        titulos.append(libro.replace("\n","").strip())

    for libro in precios_sucios:
        precios.append(libro.replace("\n","").strip())

    for libro in stocks_sucio:
        stocks.append(libro.replace("\n","").strip())
    
    siguientepag = soup.select_one("li.next>a")
    if siguientepag:
        siguienteurl = siguientepag.get("href")
        url = urljoin(url, siguienteurl)
    else:
        break


# Almacenamiento
df = pd.DataFrame({"titulo":titulos,"precio":precios,"stock":stocks})
df.head()

# Exportando los datos a un .csv
df.to_csv("libros_all.csv", index=False, encoding="utf-8")


# Importamos las librerías
import pandas as pd
from selenium import webdriver
from bs4 import BeautifulSoup as bs
from selenium.webdriver.chrome.service import Service
from webdriver_manager.chrome import ChromeDriverManager
from urllib.parse import urljoin
from selenium.webdriver.chrome.options import Options
from time import sleep

# Configuración
# Especificar la ruta del ejecutable de Brave
brave_path = "C:/Users/usuario/AppData/Local/BraveSoftware/Brave-Browser/Application/brave.exe"
driver_path = "C:/Program Files/chromedriver-win64/chromedriver.exe"  # Asegúrate de que esta es la ruta donde extrajiste el chromedriver.exe

# Configurar opciones de Chrome para usar Brave
chrome_options = Options()
chrome_options.binary_location = brave_path

# Inicia el navegador usando Brave
driver = webdriver.Chrome(service=Service(driver_path), options=chrome_options)

# Inicializamos listas para almacenar los datos
titulos = []
precios = []
stocks = []

url = "http://books.toscrape.com"

while True:
    titulos_sucios = []
    precios_sucios = []
    stocks_sucio = []

    try:
        driver.get(url)
        sleep(5)  # Espera para asegurar que la página se cargue completamente
        
        contenido = driver.page_source
        soup = bs(contenido)

        articles = soup.find_all("article", attrs={"class": "product_pod"})

        for libro in articles:
            titulo = libro.find("h3")
            titulos_sucios.append(titulo.text)

            precio = libro.find("p", attrs={"class": "price_color"})
            precios_sucios.append(precio.text)

            stock = libro.find("p", attrs={"class": "instock availability"})
            stocks_sucio.append(stock.text)

        # Limpiando los datos
        for libro in titulos_sucios:
            titulos.append(libro.replace("\n", "").strip())

        for libro in precios_sucios:
            precios.append(libro.replace("\n", "").strip())

        for libro in stocks_sucio:
            stocks.append(libro.replace("\n", "").strip())

        siguientepag = soup.select_one("li.next>a")
        if siguientepag:
            siguienteurl = siguientepag.get("href")
            url = urljoin(url, siguienteurl)
        else:
            break

    except Exception as e:
        print(f"Ocurrió un error: {e}. Saliendo.")
        break

# Almacenamiento
df = pd.DataFrame({"titulo": titulos, "precio": precios, "stock": stocks})
df.to_csv("libros_all2.csv", index=False, encoding="utf-8")

# Cierre del navegador
driver.quit()

print("Scraping completado y navegador cerrado.")

